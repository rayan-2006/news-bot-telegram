import feedparser
import asyncio
import hashlib
import re
import json
import os
import requests
from telegram import Bot
import logging
import random  

from bs4 import BeautifulSoup

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª - Ø¹ÙˆØ¶ Ú©Ù†!
BOT_TOKEN = "8297507213:AAExuYByDdP5cRaY0A0JRfMVdp9G58vj_Zs"
CHANNEL_ID = "@my_Latest_news"

# RSS feeds - Ù…Ù†Ø§Ø¨Ø¹ Ø¢Ø²Ø§Ø¯ Ø±Ùˆ Ø¨ÛŒØ´ØªØ± (ÙˆØ²Ù† Û³ Ø¨Ø±Ø§Ø¨Ø± Ø¯Ø§Ø®Ù„ÛŒ)
FREE_FEEDS = [
    "https://www.iranintl.com/rss",
    "https://ir.voanews.com/rss.xml",
    "https://www.manototv.com/rss",
    "https://www.radiofarda.com/api/zq_ottqem_tq",
    "https://rss.dw.com/rdf/rss-fa-all",
    "https://feeds.bbci.co.uk/persian/rss.xml",
    "https://www.alarabiya.net/persian/rss",
    "https://www.radiozamaneh.com/rss",
    "https://www.rfi.fr/fa/rss",  # RFI ÙØ§Ø±Ø³ÛŒ
    "https://www.euronews.com/rss/persian.xml",  # Euronews ÙØ§Ø±Ø³ÛŒ
]

DOMESTIC_FEEDS = [
    "https://www.farsnews.ir/rss",
    "https://www.tasnimnews.com/fa/rss",
    "https://www.mehrnews.com/rss",
    "https://www.isna.ir/rss",
    "https://www.irna.ir/rss",
    "https://www.eghtesadonline.com/rss",
    "https://www.donya-e-eqtesad.com/rss",
    "https://www.khabaronline.ir/rss",
]

# ÙØ§ÛŒÙ„ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ seen_hashes
SEEN_FILE = "seen_news.json"

# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ seen_hashes Ø§Ø² ÙØ§ÛŒÙ„
if os.path.exists(SEEN_FILE):
    with open(SEEN_FILE, 'r', encoding='utf-8') as f:
        seen_hashes = set(json.load(f))
else:
    seen_hashes = set()

logging.basicConfig(level=logging.INFO)
bot = Bot(token=BOT_TOKEN)

def clean_html(text):
    return re.sub('<.*?>', '', text).strip()

def get_unique_id(entry):
    pub_date = getattr(entry, 'published', '') or getattr(entry, 'updated', '')
    return hashlib.md5((entry.title + entry.link + pub_date).encode('utf-8')).hexdigest()

def save_seen():
    with open(SEEN_FILE, 'w', encoding='utf-8') as f:
        json.dump(list(seen_hashes), f)

def download_image(url):
    try:
        response = requests.get(url, timeout=10)
        if response.status_code == 200:
            return response.content
    except:
        pass
    return None

def download_video(url):
    try:
        response = requests.get(url, timeout=15)  # timeout Ø¨ÛŒØ´ØªØ± Ø¨Ø±Ø§ÛŒ ÙˆÛŒØ¯ÛŒÙˆ
        if response.status_code == 200:
            return response.content
    except:
        pass
    return None

def is_persian_text(text):
    # Ú†Ú© Ø¯Ø±ØµØ¯ Ø­Ø±ÙˆÙ ÙØ§Ø±Ø³ÛŒ/Ø¹Ø±Ø¨ÛŒ (Ø­Ø¯Ø§Ù‚Ù„ Û¶Û°%)
    persian_chars = re.findall(r'[\u0600-\u06FF]', text)
    total_chars = len(text)
    if total_chars == 0:
        return False
    return len(persian_chars) / total_chars >= 0.6  # Û¶Û°% ÙØ§Ø±Ø³ÛŒ

async def send_news():
    # ÙˆØ²Ù†â€ŒØ¯Ø§Ø± Ø±Ù†Ø¯ÙˆÙ…: Ø¢Ø²Ø§Ø¯ ÙˆØ²Ù† Û³ØŒ Ø¯Ø§Ø®Ù„ÛŒ Û± (Ø¨ÛŒØ´ØªØ± Ø¢Ø²Ø§Ø¯ Ø¨ÛŒØ§Ø¯)
    combined_feeds = FREE_FEEDS * 3 + DOMESTIC_FEEDS
    random.shuffle(combined_feeds)
    feeds_to_check = combined_feeds[:len(FREE_FEEDS + DOMESTIC_FEEDS)]  # ØªØ¹Ø¯Ø§Ø¯ Ø§ØµÙ„ÛŒ
    
    new_posts = 0
    for url in feeds_to_check:
        try:
            feed = feedparser.parse(url, request_headers={'User-Agent': 'NewsBot/1.0'})
            if not feed.entries:
                continue
            for entry in reversed(feed.entries[:10]):
                uid = get_unique_id(entry)
                if uid in seen_hashes:
                    continue
                title = entry.title.strip()
                link = entry.link.strip()
                
                # ÙÛŒÙ„ØªØ± Ø¹Ù†ÙˆØ§Ù† ÙØ§Ø±Ø³ÛŒ (Ø­Ø¯Ø§Ù‚Ù„ Û¶Û°% Ø­Ø±ÙˆÙ ÙØ§Ø±Ø³ÛŒ)
                if not is_persian_text(title):
                    continue  # skip Ø§Ú¯Ø± Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ ÛŒØ§ Ù…Ø®Ù„ÙˆØ· Ø¨ÙˆØ¯
                
                # Ù…ØªÙ† Ú©Ø§Ù…Ù„
                try:
                    # Ø§ÙˆÙ„ Ø³Ø¹ÛŒ Ú©Ù† Ø§Ø² content ÛŒØ§ summary
                    description = entry.summary if hasattr(entry, 'summary') else ""
                    content = entry.content[0].value if hasattr(entry, 'content') and entry.content else description
                    full_text = clean_html(content)
                    full_text = re.sub(r'\s+', ' ', full_text).strip()
                    
                    # Ø§Ú¯Ø± Ú©ÙˆØªØ§Ù‡ Ø¨ÙˆØ¯ØŒ Ø§Ø² Ù„ÛŒÙ†Ú© scrape Ú©Ù† (Ú©Ø§Ù…Ù„â€ŒØªØ±)
                    if len(full_text) < 800:
                        response = requests.get(link, timeout=10)
                        if response.status_code == 200:
                            soup = BeautifulSoup(response.text, 'html.parser')
                            # Ø­Ø°Ù Ø¬Ø¯ÙˆÙ„â€ŒÙ‡Ø§ØŒ ØªØ¨Ù„ÛŒØºØ§Øª Ùˆ Ø§Ø³Ú©Ø±ÛŒÙ¾Øªâ€ŒÙ‡Ø§
                            for table in soup.find_all('table'):
                                table.decompose()
                            for ad in soup.find_all('div', class_=re.compile(r'ad|advert')):
                                ad.decompose()
                            for script in soup.find_all('script'):
                                script.decompose()
                            # Ø³Ø¹ÛŒ Ú©Ù† Ø§Ø² article ÛŒØ§ body div
                            article = soup.find('article') or soup.find('div', class_='body') or soup.find('div', id='body') or soup.find('div', class_='content') or soup.find('div', id='content')
                            if article:
                                paragraphs = article.find_all('p')
                            else:
                                paragraphs = soup.find_all('p')
                            full_text = ' '.join([p.get_text().strip() for p in paragraphs[:30]])  # Û³Û° Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù Ø¨Ø±Ø§ÛŒ Ú©Ø§Ù…Ù„â€ŒØªØ±
                            full_text = clean_html(full_text)
                            full_text = re.sub(r'\s+', ' ', full_text).strip()
                    
                    # Ø¨Ø±Ø´ Ø¨Ø¯ÙˆÙ† "Ø§Ø¯Ø§Ù…Ù‡ Ø¯Ø± Ù…Ù†Ø¨Ø¹" (ÙÙ‚Ø· ... Ø§Ú¯Ø± Ù„Ø§Ø²Ù…)
                    if len(full_text) > 4000:
                        full_text = full_text[:4000] + " ..."
                except:
                    full_text = "Ù…ØªÙ† Ú©Ø§Ù…Ù„ Ø¯Ø± Ù…Ù†Ø¨Ø¹ Ù…ÙˆØ¬ÙˆØ¯ Ø§Ø³Øª."
                # Ø¹Ú©Ø³ ÛŒØ§ ÙˆÛŒØ¯ÛŒÙˆ - Ø¯Ø§Ù†Ù„ÙˆØ¯ Ú©Ù†
                media_data = None
                media_type = None  # 'photo' ÛŒØ§ 'video'
                if hasattr(entry, 'media_content'):
                    for media in entry.media_content:
                        if 'url' in media:
                            if 'jpg' in media['url'] or 'png' in media['url']:
                                media_data = download_image(media['url'])
                                media_type = 'photo'
                                break
                            elif 'mp4' in media['url'] or 'video' in media.get('type', ''):
                                media_data = download_video(media['url'])
                                media_type = 'video'
                                break
                elif hasattr(entry, 'enclosures'):
                    for enc in entry.enclosures:
                        if enc.type.startswith('image/'):
                            media_data = download_image(enc.url)
                            media_type = 'photo'
                            break
                        elif enc.type.startswith('video/'):
                            media_data = download_video(enc.url)
                            media_type = 'video'
                            break
                # Ù¾ÛŒØ§Ù… - Ø¹Ù†ÙˆØ§Ù† Ø¨ÙˆÙ„Ø¯
                caption = f"ğŸŸ¥ <b>{title}</b>\n\n{full_text}\n\n@my_Latest_news"
                try:
                    if media_data:
                        if media_type == 'video':
                            await bot.send_video(chat_id=CHANNEL_ID, video=media_data, caption=caption[:1024], parse_mode='HTML')
                            print(f"âœ… ÙˆÛŒØ¯ÛŒÙˆ Ø§Ø±Ø³Ø§Ù„ Ø´Ø¯: {title[:50]}...")
                        else:
                            await bot.send_photo(chat_id=CHANNEL_ID, photo=media_data, caption=caption[:1024], parse_mode='HTML')
                            print(f"âœ… Ø¹Ú©Ø³ Ø§Ø±Ø³Ø§Ù„ Ø´Ø¯: {title[:50]}...")
                    else:
                        if len(caption) > 4096:
                            parts = [caption[i:i+4000] for i in range(0, len(caption), 4000)]
                            for part in parts:
                                await bot.send_message(chat_id=CHANNEL_ID, text=part, parse_mode='HTML', disable_web_page_preview=False)
                                await asyncio.sleep(1)
                        else:
                            await bot.send_message(chat_id=CHANNEL_ID, text=caption, parse_mode='HTML', disable_web_page_preview=False)
                        print(f"âœ… Ù…ØªÙ† Ø§Ø±Ø³Ø§Ù„ Ø´Ø¯: {title[:50]}...")
                    seen_hashes.add(uid)
                    save_seen()
                    new_posts += 1
                    await asyncio.sleep(2)
                except Exception as e:
                    print(f"âŒ Ø®Ø·Ø§ Ø§Ø±Ø³Ø§Ù„: {e}")
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ ÙÛŒØ¯ {url}: {e}")
    print(f"ğŸ”„ Ú†Ú© ØªÙ…Ø§Ù… | Ø§Ø®Ø¨Ø§Ø± Ø¬Ø¯ÛŒØ¯: {new_posts} | Ú©Ù„: {len(seen_hashes)}")

async def main_loop():
    print("ğŸš€ Ø±Ø¨Ø§Øª Ø®Ø¨Ø±Ø®ÙˆØ§Ù† Ø´Ø±ÙˆØ¹ Ø´Ø¯...")
    try:
        while True:
            await send_news()
            await asyncio.sleep(60)  # Ù‡Ø± Û¶Û° Ø«Ø§Ù†ÛŒÙ‡
    except KeyboardInterrupt:
        print("ğŸ›‘ ØªÙˆÙ‚Ù...")
        save_seen()
    finally:
        await bot.shutdown()

if __name__ == "__main__":
    asyncio.run(main_loop())